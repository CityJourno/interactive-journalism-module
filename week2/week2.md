Interactive Journalism - Advanced Data & Coding module
========================================================

Module code: JOM299

## Intro to Python

Running your Pythons
========================================================

A Python script is a file, eg `example.py`

You run this file through the **command line** with 

> `python example.py`

Variables
========================================================
### Strings
> `variable = 'some text'`  

> `print variable`  
`> some text`

### Integers
> `variable = 1`  

> `print variable`  
`> 1`

Variables 2
========================================================
### Lists
> `list= [1, 2, 'basile']`

> `print list`  
`> [1,2,'basile']`


> `print list[0]`  
`> 1`

> `print list[1]`  
`> 2`

Variables 3
========================================================
### Dictionaries
> `addresses = {'Mum': '07439487463', 'Donal Trump': '573-555-5555'}`  

> `print addresses['Mum']`  
`> 07439487463`

Conditional execution: `if/else`
========================================================
> `name = 'basile'`  
> `if name is 'basile'`
`   print 'okay!'`

> `> okay!`

Conditional execution: `if/else`
========================================================

> `number = 10`  
> `if number > 5:`  
`    print "Wow, that's a big number!"`

## A word about booleans
In Python, they are `True` and `False`

Exercise:

* `1 == 1`
* `"test" != "testing"`
* `"test" == 1`

Control flow: `for` loop
========================================================

> `list_of_letters = ['a', 'b', 'c']`  

> `for letter in list_of_letters:`  
> `....print letter`

Methods and functions
========================================================

> `def add_two(x):`  
> `....return x + 2`  

> `var = 1`  
> `print var`  
`> 1`

> `add_two(var)`  
`> 3`

> `add_two('basile')`  
`> ERROR`

Modules
========================================================

`import module`

Scraping
========================================================

> Scraping is a way of retrieving data from websites and placing it in a simple and flexible format so it can be cross-analyzed more easily. Many times the information necessary to support a story is available, however it is found in websites that are hard to navigate or in a data base that is hard to use. To automatically collect and display this information, reporters need to turn to computer programs known as "scrapers."

https://knightcenter.utexas.edu/blog/00-9676-unraveling-data-scraping-understanding-how-scrape-data-can-facilitate-journalists-work

Scraping
========================================================

A data pipeline with Python and D3
http://kyrandale.com/static/talks/reveal.js/index_pydata2015.html#/

Scraping tools: http://knightlab.northwestern.edu/2014/03/20/five-data-scraping-tools-for-would-be-data-journalists/

Scraping
========================================================
Data Journalism Handbook: http://datajournalismhandbook.org/1.0/en/getting_data_3.html
>
The goal for most of these methods is to get access to machine-readable data. Machine readable data is created for processing by a computer, instead of the presentation to a human user. The structure of such data relates to contained information, and not the way it is displayed eventually. Examples of easily machine-readable formats include CSV, XML, JSON and Excel files, while formats like Word documents, HTML pages and PDF files are more concerned with the visual layout of the information. PDF for example is a language which talks directly to your printer, it’s concerned with position of lines and dots on a page, rather than distinguishable characters.

Scraping
========================================================

Painful lessons in data journalism: scraping with Python
https://zoomata.com/archive/painful-lessons-in-data-journalism-scraping-with-python/

> But journalism isn’t about getting “Hello World!” to run. It’s about getting your particular story to “run” correctly.

> Writing a scraper for not just one URL but over a hundred (some states have multiple sites) is rough. This is also the pain of a site without an API. (In contrast, my graph of “fiscal cliff” mentions to test out the New York Times API took about 10 minutes.)

> You hit a wall. You read the documentation. You tinker. Still doesn’t work. You pester a friend who tells you where to look in the documentation. No luck. You plug in an answer to a similar problem you read about in Stack Overflow. That does it! Rinse. Repeat. Until it actually gathers the information you need.

Scraping exercises
========================================================

101 exercises, increasing difficulty:  
https://github.com/stanfordjournalism/search-script-scrape


Your first web scraper, IRE tutorial:  
https://first-web-scraper.readthedocs.io/en/latest/#

Reading
========================================================

Variables and operations: http://nbviewer.jupyter.org/github/rajathkumarmp/Python-Lectures/blob/master/01.ipynb

Data structures: List, indexing, slicing, built-in functions: http://nbviewer.jupyter.org/github/rajathkumarmp/Python-Lectures/blob/master/03.ipynb

Strings: Built-in functions: http://nbviewer.jupyter.org/github/rajathkumarmp/Python-Lectures/blob/master/04.ipynb

Reading
========================================================
Control flow: if and loops: http://nbviewer.jupyter.org/github/rajathkumarmp/Python-Lectures/blob/master/05.ipynb

Functions: http://nbviewer.jupyter.org/github/rajathkumarmp/Python-Lectures/blob/master/06.ipynb

Calling bullshit in visual journalism: http://callingbullshit.org/syllabus.html#Visual
